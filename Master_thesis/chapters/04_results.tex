\chapter{Results and Discussion} \label{chapter4}

This chapter delivers an exhaustive examination of the multi-output neural ensemble framework applied to equity forecasting across a universe of 106 Indian listed securities. The analysis encompasses algorithmic performance quantification, feature construction efficacy assessment, directional prediction precision, and practical takeaways emerging from chronologically-ordered backtesting conducted on a decade of authentic trading records (2015--2025).

\section{Dataset and Stock Selection}

\subsection{Stock Universe: Sectoral Diversification}

The forecasting framework underwent evaluation on \textbf{106 deliberately curated equities} listed on India's premier securities exchange (NSE), spanning heterogeneous industry verticals to validate broad applicability and transferability. Table~\ref{tab:sector_distribution} details the cross-sectoral allocation.

\begin{table}[H]
\centering
\caption{Sectoral Distribution of 106 Stocks in Portfolio}
\label{tab:sector_distribution}
\renewcommand{\arraystretch}{1.5}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l c c l}
\toprule
\textbf{Sector} & \textbf{Stocks} & \textbf{\%} & \textbf{Key Representatives} \\
\midrule
Banking \& Financial Services & 12 & 11.3 & HDFCBANK, ICICIBANK, SBIN, AXISBANK \\
Information Technology & 10 & 9.4 & TCS, INFY, WIPRO, HCLTECH \\
Pharmaceuticals \& Healthcare & 9 & 8.5 & SUNPHARMA, DRREDDY, CIPLA, LUPIN \\
Automotive \& Auto Components & 9 & 8.5 & MARUTI, M\&M, TATAMOTORS, BAJAJ-AUTO \\
Energy, Oil \& Gas & 11 & 10.4 & RELIANCE, ONGC, BPCL, NTPC \\
Metals \& Mining & 9 & 8.5 & TATASTEEL, JSWSTEEL, HINDALCO, VEDL \\
Consumer Goods \& FMCG & 11 & 10.4 & HINDUNILVR, ITC, BRITANNIA, NESTLEIND \\
Construction \& Real Estate & 6 & 5.7 & LT, DLF, GODREJPROP, OBEROIRLTY \\
Cement \& Building Materials & 6 & 5.7 & ULTRACEMCO, SHREECEM, AMBUJACEM, ACC \\
Telecom \& Communication & 3 & 2.8 & BHARTIARTL, IDEA, TATACOMM \\
Diversified \& Others & 20 & 18.9 & ADANIENT, APOLLOHOSP, BAJFINANCE, TITAN \\
\midrule
\textbf{Total Universe} & \textbf{106} & \textbf{100.0} & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{0.3cm}

\small
\textit{Note: Stock selection based on NSE NIFTY 50, NIFTY 100, and sectoral indices. All stocks have minimum 10-year historical data (2015--2025) with average daily trading volume $>$ 100,000 shares.}
\end{table}

\clearpage

\textbf{Selection Criteria:}
\begin{itemize}
    \item \textbf{Liquidity:} Securities demonstrating robust daily turnover exceeding one hundred thousand units to guarantee adequate market depth and minimize execution friction
    \item \textbf{Market Capitalization:} Mid-cap to large-cap stocks (greater than INR 5,000 crores)
    \item \textbf{Data Quality:} Complete OHLCV data available for 2015--2025 period
    \item \textbf{Sectoral Balance:} Representation from all major NSE sectors
    \item \textbf{Index Constituents:} Primarily NIFTY 50, NIFTY 100, and sectoral indices
\end{itemize}

\subsection{Data Characteristics}

\begin{itemize}
    \item \textbf{Timeframe:} January 2015 to December 2025 (10 years)
    \item \textbf{Data Points:} Average 2,500--2,700 trading days per stock
    \item \textbf{Features:} 244 engineered features per stock (detailed in Section~\ref{sec:features})
    \item \textbf{Targets:} 4 prediction targets -- Close, High, Low prices, Direction (binary)
    \item \textbf{Validation:} Walk-forward validation (60\% train, 20\% validation, 20\% test)
\end{itemize}

\section{Feature Engineering Framework} \label{sec:features}

The prediction system leverages \textbf{244 professionally engineered features} per stock, categorized into 8 domains:

\begin{table}[H]
\centering
\caption{Feature Engineering Architecture (244 Total Features)}
\label{tab:features}
\renewcommand{\arraystretch}{1.5}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l c c l}
\toprule
\textbf{Feature Category} & \textbf{Count} & \textbf{\%} & \textbf{Key Components} \\
\midrule
Technical Indicators & 87 & 35.7 & SMA, EMA, MACD, RSI, Bollinger, ATR, ADX, Stochastic \\
Price-Based Features & 24 & 9.8 & Returns, Log returns, OHLC ratios, Gap analysis, VWAP \\
Volatility Measures & 18 & 7.4 & Historical, Parkinson, Garman-Klass, Yang-Zhang volatility \\
Volume Analysis & 22 & 9.0 & OBV, CMF, Volume MA, Volume RSI, A/D Line, VWAP \\
Market Regime & 31 & 12.7 & Trend strength, Volatility regime, Support/resistance \\
Temporal Features & 12 & 4.9 & Day of week, Month, Quarter, Holiday proximity \\
Sentiment Features & 15 & 6.1 & News sentiment (VADER), Sentiment momentum \\
Interaction Features & 35 & 14.3 & RSI$\times$MACD, Price$\times$Volume, Cross-indicator \\
\midrule
\textbf{Total Features} & \textbf{244} & \textbf{100} & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{0.3cm}

\small
\textit{Note: Features computed using \texttt{ta-lib} and \texttt{pandas-ta} libraries. Correlation threshold of 0.95 applied to remove redundant features. Final feature set validated through Recursive Feature Elimination (RFE) with XGBoost.}
\end{table}

\clearpage

\textbf{Feature Selection Process:}
\begin{enumerate}
    \item Correlation analysis to remove redundant features (threshold: 0.95)
    \item Recursive Feature Elimination (RFE) with XGBoost
    \item Domain expertise validation for market relevance
    \item Walk-forward feature stability testing
\end{enumerate}

\section{Model Architecture and Training}

\subsection{Multi-Target Ensemble System}

Four distinct models were trained for each stock, each predicting 4 targets simultaneously:

\begin{table}[H]
\centering
\small
\caption{Model Architecture Specifications}
\label{tab:model_arch}
\renewcommand{\arraystretch}{1.35}

\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{Model} & \textbf{Architecture Details} \\
\midrule

\textbf{XGBoost} & 
200 boosted trees, maximum depth of 5, learning rate of 0.01, 80\% row/column subsampling, and early stopping for convergence control. \\

\textbf{LSTM} & 
Two recurrent layers (128 → 64 hidden units), 30\% dropout, ten-day sequential window, Adam optimizer, and a 50-epoch training cycle. \\

\textbf{GRU} & 
Two-layer GRU stack (128 → 64 units), dropout rate 0.3, 10-step temporal context, trained for 50 epochs with Adam optimization. \\

\textbf{Ensemble} & 
Ridge-regularized meta-learner combining predictions from XGBoost, LSTM, and GRU to generate consensus forecasts. \\

\bottomrule
\end{tabularx}
\end{table}


\clearpage

\subsection{Training Configuration}

\begin{itemize}
    \item \textbf{Validation Strategy:} Walk-forward (rolling window) -- train on past, validate on next period, test on unseen future
    \item \textbf{Data Split:} 60\% training, 20\% validation, 20\% testing (chronological)
    \item \textbf{Regularization:} Dropout (0.3), L2 regularization, early stopping
    \item \textbf{Hardware:} Trained on CPU (multi-core processing), average 4--5 minutes per stock
    \item \textbf{Feature Scaling:} StandardScaler applied independently for each fold
\end{itemize}

\section{Comprehensive Performance Analysis}

\subsection{Overall Model Performance Summary}

Table~\ref{tab:overall_performance} presents aggregated metrics across all 106 stocks:

\begin{table}[H]
\centering
\caption{Overall Model Performance Across 106 Stocks}
\label{tab:overall_performance}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Dir. Acc.} & \textbf{Close $R^{2}$} & \textbf{RMSE (\%)} & \textbf{MAE (\%)} & \textbf{Avg Test} \\ \hline
XGBoost & \textbf{68.22\%} & 0.0178 & 1.38\% & 1.02\% & 512 \\ \hline
LSTM & 50.31\% & $-$0.0027 & 1.39\% & 1.03\% & 512 \\ \hline
GRU & 50.28\% & $-$0.0026 & 1.39\% & 1.03\% & 512 \\ \hline
Ensemble & \textbf{68.28\%} & \textbf{0.0270} & \textbf{1.37\%} & \textbf{1.01\%} & 512 \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}


\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Ensemble outperforms} all individual models with 68.28\% direction accuracy and 0.0270 $R^{2}$
    \item \textbf{XGBoost demonstrates strong baseline} performance (68.22\% accuracy)
    \item \textbf{LSTM and GRU show overfitting tendency} with near-random direction prediction (approximately 50\%)
    \item \textbf{Ensemble stacking successfully combines} XGBoost's feature learning with neural network patterns
\end{itemize}

\subsection{Best Model Distribution}

Analysis of which model achieved highest direction accuracy per stock:

\begin{table}[H]
\centering
\caption{Best Model Distribution by Stock (N=106)}
\label{tab:best_model_dist}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Number of Stocks} & \textbf{Percentage} \\ \hline
Ensemble & 58 & 54.7\% \\ \hline
XGBoost & 46 & 43.4\% \\ \hline
LSTM & 1 & 0.9\% \\ \hline
GRU & 1 & 0.9\% \\ \hline
\textbf{Total} & \textbf{106} & \textbf{100\%} \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Insight:} Ensemble achieves best performance on 58/106 stocks (54.7\%), validating the stacking approach. XGBoost remains competitive on 43.4\% of stocks, particularly in high-volatility sectors.

\subsection{Performance by Target Variable}

\begin{table}[H]
\centering
\caption{Model Performance by Prediction Target (Average across 106 stocks)}
\label{tab:target_performance}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Target} & \textbf{XGBoost} & \textbf{LSTM} & \textbf{GRU} & \textbf{Ensemble} \\ \hline
\multicolumn{5}{|c|}{\textbf{Closing Price Return}} \\ \hline
$R^{2}$ & 0.0178 & $-$0.0027 & $-$0.0026 & \textbf{0.0270} \\ \hline
RMSE (\%) & 1.38 & 1.39 & 1.39 & \textbf{1.37} \\ \hline
MAE (\%) & 1.02 & 1.03 & 1.03 & \textbf{1.01} \\ \hline
\multicolumn{5}{|c|}{\textbf{High Price Return}} \\ \hline
$R^{2}$ & $-$0.0412 & $-$0.0687 & $-$0.0686 & $-$0.0821 \\ \hline
RMSE (\%) & 1.09 & 1.11 & 1.11 & 1.13 \\ \hline
MAE (\%) & 0.82 & 0.83 & 0.83 & 0.85 \\ \hline
\multicolumn{5}{|c|}{\textbf{Low Price Return}} \\ \hline
$R^{2}$ & \textbf{0.0089} & $-$0.0421 & $-$0.0419 & $-$0.0315 \\ \hline
RMSE (\%) & \textbf{1.03} & 1.05 & 1.05 & 1.04 \\ \hline
MAE (\%) & \textbf{0.75} & 0.77 & 0.77 & 0.76 \\ \hline
\multicolumn{5}{|c|}{\textbf{Direction (Classification)}} \\ \hline
Accuracy & 68.22\% & 50.31\% & 50.28\% & \textbf{68.28\%} \\ \hline
Precision & 66.31\% & 50.31\% & 50.28\% & \textbf{66.54\%} \\ \hline
Recall & 74.82\% & 100.0\% & 100.0\% & \textbf{75.12\%} \\ \hline
F1-Score & 0.6954 & 0.6686 & 0.6685 & \textbf{0.7024} \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}

\section{Detailed Results: Representative Stock Analysis}

\subsection{Case Study: RELIANCE (Reliance Industries Limited)}

RELIANCE was selected as a representative example from the Energy sector, being India's largest private sector company with significant market impact.

\subsubsection{Stock-Specific Performance Metrics}

\begin{table}[H]
\centering
\caption{RELIANCE: Model Performance Comparison}
\label{tab:reliance_performance}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Features} & \textbf{Dir. Acc.} & \textbf{Close $R^{2}$} & \textbf{RMSE (\%)} & \textbf{F1-Score} \\ \hline
XGBoost & 244 & 71.62\% & $-$0.2963 & 1.51\% & 0.7306 \\ \hline
LSTM & 244 & 52.62\% & $-$0.0004 & 1.33\% & 0.6896 \\ \hline
GRU & 244 & 52.62\% & $-$0.0002 & 1.33\% & 0.6896 \\ \hline
Ensemble & 244 & \textbf{72.23\%} & \textbf{0.0114} & \textbf{1.32\%} & \textbf{0.7366} \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Analysis:} RELIANCE demonstrates the Ensemble advantage clearly -- achieving 72.23\% direction accuracy compared to random baseline (50\%). The positive $R^{2}$ of 0.0114 indicates genuine predictive power despite stock market volatility.

\subsubsection{Prediction Visualizations}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../plots/RELIANCE_comparison_plot.png}
\caption{RELIANCE: Actual vs. Predicted Prices for All Models}
\label{fig:reliance_comparison}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../plots/RELIANCE_confusion_matrices.png}
\caption{RELIANCE: Confusion Matrices for Direction Prediction}
\label{fig:reliance_confusion}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../plots/RELIANCE_roc_curves.png}
\caption{RELIANCE: ROC Curves for Direction Prediction}
\label{fig:reliance_roc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../plots/RELIANCE_precision_recall_curves.png}
\caption{RELIANCE: Precision-Recall Curves}
\label{fig:reliance_precision_recall}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../plots/RELIANCE_feature_importance.png}
\caption{RELIANCE: Top 30 Feature Importance from XGBoost}
\label{fig:reliance_feature_importance}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../plots/RELIANCE_error_distribution.png}
\caption{RELIANCE: Prediction Error Distribution for Close Price}
\label{fig:reliance_error_dist}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../plots/RELIANCE_prediction_scatter.png}
\caption{RELIANCE: Actual vs. Predicted Close Prices}
\label{fig:reliance_scatter}
\end{figure}

\section{Aggregate Analysis Across All Stocks}

\subsection{Model Performance Comparison}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../plots/COMBINED_model_performance.png}
\caption{Model Performance Distribution Across 106 Stocks}
\label{fig:combined_performance}
\end{figure}

\textbf{Key Insights:}
\begin{itemize}
    \item Ensemble and XGBoost maintain consistent performance across diverse stocks (low variance)
    \item LSTM/GRU show high variance and tendency to overfit on training data (wide IQR in box plots)
    \item Median direction accuracy of 68\% represents 36\% improvement over random baseline (50\%)
    \item Ensemble achieves positive median $R^{2}$ (0.0270), indicating genuine predictive value beyond mean baseline
\end{itemize}

\subsection{Direction Accuracy Distribution}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../plots/COMBINED_direction_accuracy_distribution.png}
\caption{Direction Accuracy Distribution Across 106 Stocks}
\label{fig:combined_direction_acc}
\end{figure}

\subsection{$R^{2}$ Score Distribution by Target}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../plots/COMBINED_r2_distribution.png}
\caption{$R^{2}$ Score Distribution for Close, High, and Low Price Predictions}
\label{fig:combined_r2}
\end{figure}

\subsection{Best Model Selection by Metric}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../plots/COMBINED_best_model_distribution.png}
\caption{Best Model Distribution by Performance Metric}
\label{fig:combined_best_models}
\end{figure}

\subsection{Performance Heatmap}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth, trim=0 1100 0 0, clip]{../plots/COMBINED_performance_heatmap.png}
\caption{Direction Accuracy Heatmap: Stocks 1--53 (Top Half)}
\label{fig:combined_heatmap_top}
\end{figure}

\clearpage

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth, trim=0 0 0 1100, clip]{../plots/COMBINED_performance_heatmap.png}
\caption{Direction Accuracy Heatmap: Stocks 54--106 (Bottom Half)}
\label{fig:combined_heatmap_bottom}
\end{figure}

\section{Why XGBoost Performed Exceptionally Well}

XGBoost emerged as a strong baseline model, achieving 68.22\% average direction accuracy. Several factors contribute to its effectiveness:

\subsection{Algorithmic Advantages}

\begin{enumerate}
    \item \textbf{Iterative Error Correction:} The XGBoost framework constructs a committee of shallow tree structures in succession, where each subsequent tree specifically targets the prediction errors remaining from its predecessors. This cumulative approach excels at capturing the complex, non-linear dynamics inherent in equity price movements.
    
    \item \textbf{Complexity Control:} The algorithm incorporates both absolute-value and squared-magnitude penalties on model coefficients, effectively constraining model complexity to prevent memorization of random fluctuations in historical market data while preserving predictive capability on future unseen periods.
    
    \item \textbf{Feature Importance:} Gain-based feature importance identifies most predictive features (technical indicators, sentiment scores, volatility measures), enabling interpretable predictions.
    
    \item \textbf{Handling Missing Data:} XGBoost learns optimal strategies for missing values (common in financial time series), unlike neural networks requiring complete data.
    
    \item \textbf{Tree-Based Splits:} Decision boundaries capture non-linear market regimes (bull, bear, sideways) better than linear combinations.
\end{enumerate}

\subsection{Financial Data Compatibility}

\begin{itemize}
    \item \textbf{Heterogeneous Features:} XGBoost efficiently processes 244 features of varying scales (prices, ratios, indicators, sentiment) without extensive normalization.
    
    \item \textbf{Categorical Handling:} Naturally processes temporal features (day of week, month) and market regime indicators.
    
    \item \textbf{Outlier Robustness:} Tree-based splits are resilient to extreme price movements (flash crashes, circuit breakers).
    
    \item \textbf{Interaction Discovery:} Automatically detects interaction effects (e.g., RSI+MACD combinations) without manual feature engineering.
\end{itemize}

\subsection{Comparison with Neural Networks}

LSTM and GRU underperformed (50\% accuracy) due to:
\begin{itemize}
    \item \textbf{Overfitting:} High capacity neural networks memorize training patterns without generalizing
    \item \textbf{Sequential Dependency:} Strict temporal dependencies fail when market regime changes abruptly
    \item \textbf{Hyperparameter Sensitivity:} Require extensive tuning (learning rate, dropout, sequence length)
    \item \textbf{Data Requirements:} Need larger datasets than available per stock
\end{itemize}

XGBoost avoids these pitfalls through simpler architecture and regularization.

\section{Ensemble Model: Strengths and Future Improvements}

\subsection{Current Strengths}

The stacking ensemble achieves \textbf{68.28\% direction accuracy} and \textbf{0.0270 $R^{2}$}, outperforming individual models:

\begin{enumerate}
    \item \textbf{Complementary Learning:} Combines XGBoost's feature learning with LSTM/GRU's temporal patterns
    \item \textbf{Error Correction:} Meta-learner (Ridge Regression) corrects individual model biases
    \item \textbf{Robustness:} Diversification across model types reduces variance
    \item \textbf{Adaptive Predictions:} Weights adjust based on recent performance
\end{enumerate}

\subsection{Identified Limitations}

\begin{itemize}
    \item \textbf{LSTM/GRU Contribution Minimal:} Neural networks contribute little due to approximately 50\% accuracy
    \item \textbf{Simple Meta-Learner:} Ridge Regression may not capture complex model interactions
    \item \textbf{Equal Weighting:} No dynamic adjustment based on market conditions
    \item \textbf{Computational Cost:} Training 4 models per stock increases runtime
\end{itemize}

\subsection{Proposed Future Enhancements}

\subsubsection{Advanced Neural Architectures}

\begin{itemize}
    \item \textbf{Transformer Models:} Replace LSTM/GRU with Temporal Fusion Transformers (TFT) for better long-range dependencies
    \item \textbf{Attention Mechanisms:} Learn which features matter at which time steps
    \item \textbf{TCN (Temporal Convolutional Networks):} Faster training with receptive fields matching market cycles
\end{itemize}

\subsubsection{Improved Meta-Learning}

\begin{itemize}
    \item \textbf{Stacked Generalization:} Multi-level stacking with gradient boosting as meta-learner
    \item \textbf{Dynamic Weighting:} Time-varying model weights based on recent accuracy
    \item \textbf{Confidence-Based Voting:} Weight predictions by model uncertainty estimates
    \item \textbf{Market Regime Detection:} Switch between models based on volatility/trend state
\end{itemize}

\subsubsection{Additional Models}

\begin{itemize}
    \item \textbf{LightGBM:} Faster gradient boosting variant
    \item \textbf{CatBoost:} Better handling of categorical features
    \item \textbf{TabNet:} Attention-based deep learning for tabular data
\end{itemize}

\section{Role of News Sentiment in Prediction}

Sentiment analysis from financial news plays a \textbf{significant role} in improving direction prediction accuracy, as evidenced by feature importance analysis.

\subsection{Sentiment Feature Engineering}

\textbf{15 sentiment-based features} were incorporated:
\begin{itemize}
    \item \textbf{Raw Scores:} Positive, negative, neutral sentiment from news articles
    \item \textbf{Sentiment Momentum:} Rate of sentiment change over 1d, 5d, 20d windows
    \item \textbf{Sentiment-Price Mismatch:} Quantifies the gap between media tone and actual market behavior, flagging potential mean-reversion opportunities
    \item \textbf{Broad Market Mood:} Captures prevailing investor psychology derived from benchmark index constituents including banking and diversified portfolios
    \item \textbf{Opinion Stability:} Measures the dispersion of sentiment readings over time, indicating conviction levels in market narratives
\end{itemize}

\subsection{Impact on Predictions}

Analysis of XGBoost feature importance reveals:

\begin{itemize}
    \item \textbf{Top 30 Features Include 4--6 Sentiment Indicators:} Sentiment features consistently rank in top 30 predictors
    
    \item \textbf{Sentiment Momentum Most Important:} Rate of sentiment change predicts direction better than absolute scores
    
    \item \textbf{Complementary to Technical Indicators:} Sentiment captures fundamental shifts that technical indicators miss (e.g., earnings surprises, policy changes)
    
    \item \textbf{Lagging Effect:} 1-day lagged sentiment shows stronger correlation than same-day sentiment, as market absorbs news gradually
\end{itemize}

\clearpage

\subsection{Sector-Specific Sentiment Patterns}

\begin{table}[H]
\centering
\caption{Sentiment Impact by Sector}
\label{tab:sentiment_impact}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|c|p{5.5cm}|}
\hline
\textbf{Sector} & \textbf{Sentiment Weight} & \textbf{Key Drivers} \\ \hline
Banking & High & RBI policy, credit growth, NPA reports \\ \hline
IT & Medium & Global tech sentiment, dollar rates \\ \hline
Pharma & High & FDA approvals, clinical trial news \\ \hline
Energy & Medium & Crude oil prices, government policies \\ \hline
Consumer & Low & Stable demand, less news-driven \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{Limitations and Future Work}

\textbf{Current Limitations:}
\begin{itemize}
    \item Sentiment scores from basic NLP models (VADER, TextBlob)
    \item No company-specific news filtering (market-wide sentiment used)
    \item English-language news only (missing Hindi/regional media)
    \item Static sentiment features (no context-aware embeddings)
\end{itemize}

\textbf{Proposed Enhancements:}
\begin{itemize}
    \item \textbf{Transformer-Based Sentiment:} Use FinBERT or specialized financial language models
    \item \textbf{Entity Recognition:} Extract company-specific news with NER (Named Entity Recognition)
    \item \textbf{Multi-Source Integration:} Combine news, social media (Twitter/Reddit), analyst reports
    \item \textbf{Temporal Attention:} Model how news impact decays over time
\end{itemize}

\section{Statistical Significance and Robustness}

\subsection{Walk-Forward Validation}

Unlike traditional train-test splits, \textbf{walk-forward validation} ensures realistic performance:
\begin{itemize}
    \item Models trained only on past data
    \item Predictions made on strictly future periods
    \item No lookahead bias or data leakage
    \item Mimics real-world deployment scenario
\end{itemize}

\subsection{Performance Stability}

Table~\ref{tab:stability} shows prediction consistency:

\begin{table}[H]
\centering
\caption{Model Stability Analysis (Standard Deviation of Accuracy)}
\label{tab:stability}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Mean Accuracy} & \textbf{Std Dev} \\ \hline
XGBoost & 68.22\% & 12.3\% \\ \hline
LSTM & 50.31\% & 8.1\% \\ \hline
GRU & 50.28\% & 8.0\% \\ \hline
Ensemble & 68.28\% & 11.8\% \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Interpretation:} Ensemble maintains high accuracy (68.28\%) with moderate variability (11.8\% std), indicating robust generalization across diverse market conditions and stock behaviors.

\section{Discussion}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Ensemble Superiority:} Stacking approach achieves 68.28\% direction accuracy, significantly beating random baseline (50\%) and individual models
    
    \item \textbf{XGBoost Robustness:} Gradient boosting effectively handles 244 features and captures non-linear market dynamics
    
    \item \textbf{Neural Network Challenges:} LSTM/GRU overfit despite regularization, requiring architectural improvements
    
    \item \textbf{Feature Engineering Critical:} 244 engineered features provide rich signal, with technical indicators and sentiment driving predictions
    
    \item \textbf{Sectoral Generalization:} System performs consistently across 106 stocks spanning 11 sectors
\end{enumerate}

\subsection{Practical Implications}

\textbf{For Investors:}
\begin{itemize}
    \item 68\% direction accuracy enables profitable trading strategies with proper risk management
    \item Ensemble predictions provide confidence scores for position sizing
    \item Feature importance guides fundamental analysis focus areas
\end{itemize}

\textbf{For Researchers:}
\begin{itemize}
    \item Demonstrates effectiveness of multi-target learning for financial forecasting
    \item Validates walk-forward validation for realistic backtesting
    \item Highlights need for better neural network architectures
\end{itemize}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Transaction Costs:} Predictions assume zero costs; real-world trading includes brokerage, slippage, taxes
    
    \item \textbf{Market Impact:} Substantial trading volumes can shift prevailing prices, particularly for moderately capitalized securities where liquidity constraints amplify order-induced price movements
    
    \item \textbf{Regime Changes:} Model trained on 2015--2025 may not adapt to unprecedented events (e.g., 2008-level crisis)
    
    \item \textbf{Correlation Risk:} All stocks from Indian market -- lacks global diversification
    
    \item \textbf{Sentiment Quality:} Basic NLP sentiment may miss nuanced financial language
\end{enumerate}

\subsection{Comparison with Prior Work}

\begin{table}[H]
\centering
\caption{Performance Comparison with Literature}
\label{tab:literature_comparison}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|p{3.5cm}|c|c|c|}
\hline
\textbf{Study} & \textbf{Models} & \textbf{Accuracy} & \textbf{Features} \\ \hline
This Work & XGBoost+LSTM+GRU+Ensemble & 68.28\% & 244 \\ \hline
Shah et al. (2021) & LSTM & 56\% & 12 \\ \hline
Kumar et al. (2022) & Random Forest & 62\% & 45 \\ \hline
Singh et al. (2023) & Transformer & 59\% & 18 \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}

\clearpage

\textbf{Our Contribution:}
\begin{itemize}
    \item Largest stock universe (106 stocks vs. typical 5--20)
    \item Most comprehensive feature set (244 vs. typical 10--50)
    \item Multi-target prediction (4 targets simultaneously)
    \item Realistic walk-forward validation (vs. random train-test splits)
\end{itemize}

\section{Summary}

This chapter presented comprehensive results from the multi-target ensemble prediction system deployed on 106 Indian stocks. Key achievements include:

\begin{itemize}
    \item \textbf{68.28\% direction accuracy} with Ensemble model (36\% above random)
    \item \textbf{244 engineered features} spanning technical, fundamental, sentiment, and temporal domains
    \item \textbf{Robust performance} across 11 sectors and diverse market conditions
    \item \textbf{XGBoost excellence} attributed to gradient boosting, regularization, and tree-based learning
    \item \textbf{Sentiment integration} improving predictions through news analysis
    \item \textbf{Actionable insights} for portfolio optimization and risk management
\end{itemize}

Future enhancements focusing on advanced neural architectures (Transformers, Attention), improved meta-learning strategies, and sophisticated sentiment analysis promise to push direction accuracy toward 75\%+ while maintaining robustness.
